{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Feasibility Problems or Solving Systems of Nonlinear Equations\n",
    "\n",
    "When we use an optimization algorithm and are trying to describe the quality of a given design point in that design space, there are two adjectives that come up often. Let's define them:\n",
    "\n",
    "* *Feasibility*: This means that the point lies within the feasible region of the design space, i.e. no constraints are violated. This is also referred to as \"primal feasibility\".\n",
    "\n",
    "* *Optimality*: This means that if we move an infinitesimal amount in **any** *feasible direction* (a direction that doesn't cause us to violate constraints if we take this infinitesimal step), we won't find a point that is better. In lay terms, \"this point is better than or equal to all of its neighbors\". This is also referred to as \"dual feasibility\".\n",
    "\n",
    "Clearly, the goal of our algorithm is to find a point that is both feasible and optimal. Is this problem we're asking [well-posed](https://en.wikipedia.org/wiki/Well-posed_problem)? In other words:\n",
    "\n",
    "1. Does a solution exist?\n",
    "\n",
    "2. Is that solution unique?\n",
    "\n",
    "3. Is the solution stable w.r.t. perturbations in problem parameters?\n",
    "\n",
    "Well, in the case of a continuous, unimodal problem, the answer is generally *yes*.\n",
    "\n",
    "Sometimes, however, we're only concerned with feasibility and not optimality. Another way to state this would be that we might have a scenario where all feasible solutions are equally optimal.\n",
    "\n",
    "How would we encode this in AeroSandbox?\n",
    "\n",
    "We could either write:\n",
    "\n",
    "* `opti.minimize(0)`, which explicitly says that the performance of any feasible design is zero everywhere, and hence any feasible solution is equally optimal. We could also replace `0` with any other constant value.\n",
    "\n",
    "* We could simply not write any kind of `opti.minimize()` statement, in which case the solver will default to `opti.minimize(0)` (effectively, no objective function.)\n",
    "\n",
    "## Feasibility Problems in Design\n",
    "\n",
    "Feasibility problems can come up in the case of engineering design, albeit not often. With feasibility problems, the problem is often not well-posed, because the solution may not be unique. That doesn't mean it's unsolvable; it simply means that it's difficult to tell *a priori* what our solution will be.\n",
    "\n",
    "A code example of this follows. Here, we pose the trivial problem of finding some value of $x$ that satisfies $1 < x < 2$. We naively guess that $x = 5$:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_optimal = 1.5274119593606093\n"
     ]
    }
   ],
   "source": [
    "import aerosandbox as asb\n",
    "import aerosandbox.numpy as np\n",
    "\n",
    "opti = asb.Opti()\n",
    "\n",
    "x = opti.variable(init_guess=5)\n",
    "\n",
    "opti.subject_to([\n",
    "    x > 1,\n",
    "    x < 2,\n",
    "])\n",
    "\n",
    "sol = opti.solve(verbose=False)\n",
    "\n",
    "print(f\"x_optimal = {sol.value(x)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, we got a feasible solution! However, notice that the solution is some apparently-random (but deterministic) value between 1 and 2; it is not simply the value of the initial guess projected directly onto the nearest boundary of the feasible space (which would have given us $x \\approx 2$).\n",
    "\n",
    "In fact, here, there are infinite solutions arranged on a line between $x=1$ and $x=2$. All solutions are adjacent - take an infinitesimal step from any optimal point, and you'll get another optimal point.\n",
    "\n",
    "This same thing can happen in higher dimensions, though it can be less obvious because the \"lines\" or \"hyperplanes\" of optimality are not necessarily axis-aligned. Because of that, when you suspect you have a design problem that you expect has this kind of geometry (an infinite number of adjacent optimal solutions), you might try adding a slight regularization term (such as a very weak quadratic penalty term) in order to \"select\" a single solution from all of the optima.\n",
    "\n",
    "This can be a great tool for understanding the design space better.\n",
    "\n",
    "## Solving Nonlinear Systems of Equations\n",
    "\n",
    "This idea of a \"feasibility problem\" can also come up when solving a system of nonlinear equations. Here, we solve a system of nonlinear equations implicitly by setting their governing equations as constraints. The natural extension of this idea is something called Simultaneous Analysis and Design (SAND), but that's a bit beyond scope for just this lesson.\n",
    "\n",
    "Let's give an example of solving a system of nonlinear equations:\n",
    "\n",
    "$ y = x^2 $\n",
    "\n",
    "$ y^2 = 18 - x $\n",
    "\n",
    "This system of equations turns out to have two solutions: $(x, y) = (2, 4) = (-2.118, 4.485)$. Let's find them with AeroSandbox:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_optimal = 2.000000000004754\n",
      "y_optimal = 3.99999999999944\n"
     ]
    }
   ],
   "source": [
    "opti = asb.Opti()\n",
    "\n",
    "x = opti.variable(init_guess=5)\n",
    "y = opti.variable(init_guess=5)\n",
    "\n",
    "opti.subject_to([\n",
    "    y == x ** 2,\n",
    "    y ** 2 == 18 - x\n",
    "])\n",
    "\n",
    "sol = opti.solve(verbose=False)\n",
    "\n",
    "print(f\"x_optimal = {sol.value(x)}\")\n",
    "print(f\"y_optimal = {sol.value(y)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, we found a solution for our governing equations.\n",
    "\n",
    "We could find the other solution by trying a different initial guess:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_optimal = -2.117850972326501\n",
      "y_optimal = 4.485292740984305\n"
     ]
    }
   ],
   "source": [
    "opti = asb.Opti()\n",
    "\n",
    "x = opti.variable(init_guess=-1)\n",
    "y = opti.variable(init_guess=1)\n",
    "\n",
    "opti.subject_to([\n",
    "    y == x ** 2,\n",
    "    y ** 2 == 18 - x\n",
    "])\n",
    "\n",
    "sol = opti.solve(verbose=False)\n",
    "\n",
    "print(f\"x_optimal = {sol.value(x)}\")\n",
    "print(f\"y_optimal = {sol.value(y)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And, some initial guesses won't converge (typically, this is due to a sign flip in the constraints jacobian, driving the solution in the wrong direction):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Ipopt version 3.12.3, running with linear solver mumps.\n",
      "NOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        4\n",
      "Number of nonzeros in inequality constraint Jacobian.:        0\n",
      "Number of nonzeros in Lagrangian Hessian.............:        2\n",
      "\n",
      "Total number of variables............................:        2\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        0\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        2\n",
      "Total number of inequality constraints...............:        0\n",
      "        inequality constraints with only lower bounds:        0\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        0\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0 0.0000000e+000 1.80e+001 0.00e+000   0.0 0.00e+000    -  0.00e+000 0.00e+000   0\n",
      "   1 0.0000000e+000 5.84e+000 0.00e+000 -11.0 6.80e+000    -  1.00e+000 5.00e-001h  2\n",
      "   2 0.0000000e+000 5.89e+000 0.00e+000 -11.0 2.43e+000    -  1.00e+000 1.00e+000h  1\n",
      "   3 0.0000000e+000 5.26e+000 0.00e+000 -11.0 2.29e+000    -  1.00e+000 1.00e+000h  1\n",
      "   4 0.0000000e+000 4.33e+000 0.00e+000 -11.0 2.61e+000    -  1.00e+000 5.00e-001h  2\n",
      "   5 0.0000000e+000 4.27e+000 0.00e+000 -11.0 7.28e+000    -  1.00e+000 6.25e-002h  5\n",
      "   6 0.0000000e+000 4.25e+000 0.00e+000 -11.0 1.35e+001    -  1.00e+000 1.56e-002h  7\n",
      "   7 0.0000000e+000 4.25e+000 0.00e+000 -11.0 3.93e+001    -  1.00e+000 1.95e-003h 10\n",
      "   8 0.0000000e+000 4.25e+000 0.00e+000 -11.0 9.32e+001    -  1.00e+000 2.44e-004h 13\n",
      "   9r0.0000000e+000 4.25e+000 9.99e+002   0.6 0.00e+000    -  0.00e+000 4.77e-007R 22\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  10r0.0000000e+000 4.24e+000 9.36e+002  -5.4 9.20e-001    -  1.00e+000 6.36e-002f  1\n",
      "  11r0.0000000e+000 4.24e+000 3.46e+001  -1.9 1.87e-002    -  1.00e+000 9.93e-001f  1\n",
      "  12r0.0000000e+000 4.24e+000 3.98e-002  -3.7 1.75e-002    -  1.00e+000 9.95e-001f  1\n",
      "  13r0.0000000e+000 4.24e+000 1.97e-005  -5.6 3.05e-004    -  1.00e+000 1.00e+000h  1\n",
      "  14r0.0000000e+000 4.24e+000 2.42e-006  -9.0 2.75e-009    -  1.00e+000 1.00e+000h  1\n",
      "\n",
      "Number of Iterations....: 14\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:  0.0000000000000000e+000   0.0000000000000000e+000\n",
      "Dual infeasibility......:  5.0108496907341760e-008   5.0108496907341760e-008\n",
      "Constraint violation....:  4.2391627628041215e+000   4.2391627628041215e+000\n",
      "Complementarity.........:  0.0000000000000000e+000   0.0000000000000000e+000\n",
      "Overall NLP error.......:  4.2391627628041215e+000   4.2391627628041215e+000\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 65\n",
      "Number of objective gradient evaluations             = 11\n",
      "Number of equality constraint evaluations            = 78\n",
      "Number of inequality constraint evaluations          = 0\n",
      "Number of equality constraint Jacobian evaluations   = 17\n",
      "Number of inequality constraint Jacobian evaluations = 0\n",
      "Number of Lagrangian Hessian evaluations             = 15\n",
      "Total CPU secs in IPOPT (w/o function evaluations)   =      0.013\n",
      "Total CPU secs in NLP function evaluations           =      0.000\n",
      "\n",
      "EXIT: Converged to a point of local infeasibility. Problem may be infeasible.\n",
      "      solver  :   t_proc      (avg)   t_wall      (avg)    n_eval\n",
      "       nlp_f  |        0 (       0)        0 (       0)        65\n",
      "       nlp_g  |        0 (       0)        0 (       0)        78\n",
      "  nlp_grad_f  |        0 (       0)        0 (       0)        12\n",
      "  nlp_hess_l  |        0 (       0)        0 (       0)        14\n",
      "   nlp_jac_g  |        0 (       0)        0 (       0)        18\n",
      "       total  |  13.00ms ( 13.00ms)  12.96ms ( 12.96ms)         1\n",
      "Error in Opti::solve [OptiNode] at .../casadi/core/optistack.cpp:159:\n",
      ".../casadi/core/optistack_internal.cpp:999: Assertion \"return_success(accept_limit)\" failed:\n",
      "Solver failed. You may use opti.debug.value to investigate the latest values of variables. return_status is 'Infeasible_Problem_Detected'\n",
      "x_last = 0.059022412472569466\n",
      "y_last = -4.235679117630039\n"
     ]
    }
   ],
   "source": [
    "opti = asb.Opti()\n",
    "\n",
    "x = opti.variable(init_guess=-1)\n",
    "y = opti.variable(init_guess=-1)\n",
    "\n",
    "opti.subject_to([\n",
    "    y == x ** 2,\n",
    "    y ** 2 == 18 - x\n",
    "])\n",
    "\n",
    "try:\n",
    "    sol = opti.solve()\n",
    "except RuntimeError as e:\n",
    "    print(e)\n",
    "    print(f\"x_last = {opti.debug.value(x)}\")\n",
    "    print(f\"y_last = {opti.debug.value(y)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Anyway, the basic idea is that if you're solving a system of nonlinear equations, you are solving a feasibility problem. With certain assumptions, this feasibility problem is well-posed even without adding an objective function with `opti.minimize()`. These assumptions:\n",
    "\n",
    "* The constraint jacobian matrix is square and full-rank. What this means in non-math-speak:\n",
    "\n",
    "    * You have the same number of variables and equality constraints. (For simplicity, assume we have no inequality constraints.)\n",
    "\n",
    "    * None of your equality constraints are linearly dependent.\n",
    "\n",
    "This is usually the case when solving a system of nonlinear equations - if it is not, you don't have a unique solution anyway, regardless of what method you're using to solve your problem.\n",
    "\n",
    "(Side note: One cool thing about solving systems of nonlinear equations with an optimization approach: it's a very natural transition between solving well-posed systems of equations and finding solutions to underdetermined systems that minimize some error metric. This is sort of extending the idea of converting a matrix inverse into a Moore-Penrose pseudoinverse, but a more natural and extensible way.)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}